import logging
import time
from ..planner.interface import ReasoningEngine
from ..planner.plan_model import Plan
from ..tools.executor import ToolExecutor
from ..memory.graph import MemoryGraph
from ..memory.updater import MemoryUpdater
from ..stability.signals import StabilitySignals
from ..models.observation import Observation
from .state import AgentState
from ..memory.observation_builder import ObservationBuilder
import pdb  

logger = logging.getLogger(__name__)


class Agent:

    def __init__(self, planner: ReasoningEngine, executor: ToolExecutor):
        self.planner = planner
        self.executor = executor

        self.memory = MemoryGraph()
        self.memory_updater = MemoryUpdater(self.memory)
        self.stability = StabilitySignals(self.memory)

        self.registry = executor.registry
        self.state = AgentState()
        self.obs_builder = ObservationBuilder()

    def handle_query(self, user_input: str):

        total_start = time.time()
        logger.info(f"[AGENT] New turn: {user_input}")

        # --- Session ---
        self.state.new_turn(user_input)

        # --- User Observation ---
        user_obs = Observation(source="user", type="entity", payload=user_input, metadata={})
        self.memory_updater.update_from_observation(user_obs)

        # --- Stability ---
        t0 = time.time()
        signals = self.stability.extract()
        logger.info(f"[STABILITY] {time.time() - t0:.2f}s")

        # --- Planning ---
        t0 = time.time()
        tools = self.registry.list_tools()
        plan: Plan = self.planner.generate_plan(user_input, signals, tools)
        logger.info(f"[PLANNER] {time.time() - t0:.2f}s")

        self.state.record_plan(plan)

        if plan.is_empty():
            logger.warning("[PLANNER] Empty plan produced")
            return {"error": "Planner produced no action"}

        step = plan.first_step
        if not step.action:
            logger.warning("[PLANNER] Invalid plan step")
            return {"error": "Invalid plan step"}

        self.state.last_tool_call = step.action

        # --- Execution ---
        logger.info(f"[EXECUTOR] Calling tool: {step.action.tool_name}")
        pdb.set_trace()
        t0 = time.time()
        result = self.executor.execute(step.action.tool_name, step.action.arguments)
        logger.info(f"[EXECUTOR] {time.time() - t0:.2f}s")

        self.state.record_execution(step.action, result)

        # --- Store Raw Result ---
        tool_obs = Observation(source="tool", type="result", payload=result, metadata={})
        self.memory_updater.update_from_observation(tool_obs)

        # --- Semantic Knowledge Encoding ---
        if (
            result.tool_name == "reason"
            and getattr(result, "status", None) == "success"
            and isinstance(getattr(result, "output", None), dict)
            and "answer" in result.output
        ):
            logger.info("[SEMANTIC] Extracting structured knowledge")
            t0 = time.time()
            answer_text = result.output["answer"]
            semantic_observations = self.obs_builder.from_reason_result(answer_text)
            logger.info(f"[SEMANTIC] {time.time() - t0:.2f}s")

            for obs in semantic_observations:
                self.memory_updater.update_from_observation(obs)

        logger.info(f"[TOTAL TURN] {time.time() - total_start:.2f}s")
        logger.debug(f"Execution result: {result}")

        return result
from dataclasses import dataclass, field
from typing import List, Optional

from ..models.tool_call import ToolCall
from ..models.tool_result import ToolResult
from ..planner.plan_model import Plan


@dataclass
class AgentState:
    """
    Represents the mutable runtime state of an agent session.

    This is NOT long-term memory. That belongs to the memory graph.
    AgentState tracks short-term execution context.
    """

    # ------------------------------------------------------------------
    # Conversation Context
    # ------------------------------------------------------------------

    turn_count: int = 0
    """
    Number of turns processed in this session.
    """

    last_user_input: Optional[str] = None
    """
    Most recent user input.
    """

    # ------------------------------------------------------------------
    # Planning Context
    # ------------------------------------------------------------------

    last_plan: Optional[Plan] = None
    """
    Last generated plan (useful for retries or stability checks).
    """

    # ------------------------------------------------------------------
    # Execution Context
    # ------------------------------------------------------------------

    last_tool_call: Optional[ToolCall] = None
    """
    Last tool invoked.
    """

    last_result: Optional[ToolResult] = None
    """
    Last tool execution result.
    """

    # ------------------------------------------------------------------
    # Short-Term History (bounded)
    # ------------------------------------------------------------------

    recent_results: List[ToolResult] = field(default_factory=list)
    """
    Short-term result history for immediate context.
    Not persisted long-term.
    """

    max_recent: int = 5
    """
    Maximum number of recent results to keep.
    """

    # ------------------------------------------------------------------
    # State Update Helpers
    # ------------------------------------------------------------------

    def new_turn(self, user_input: str) -> None:
        """Advance session turn and store latest user input."""
        self.turn_count += 1
        self.last_user_input = user_input

    def record_plan(self, plan: Plan) -> None:
        """Store last generated plan."""
        self.last_plan = plan

    def record_execution(self, tool_call: ToolCall, result: ToolResult) -> None:
        """
        Store execution details and maintain bounded history.
        """
        self.last_tool_call = tool_call
        self.last_result = result

        self.recent_results.append(result)
        if len(self.recent_results) > self.max_recent:
            self.recent_results.pop(0)
from typing import Optional

from .agent.core import Agent
from .tools.executor import ToolExecutor
from .tools.registry import ToolRegistry
from .connectors.manager import ConnectorManager
from .planner.factory import create_planner
from .config import AgentConfig


class TopoMindApp:
    """
    Top-level framework facade for constructing a TopoMind Agent.

    This class represents the **official public entry point** of the
    TopoMind framework. It hides internal wiring complexity and enforces
    a clean separation between:

        • Framework-owned cognition (agent, planner, memory, execution)
        • Consumer-owned infrastructure (tools and connectors)

    Design Principles
    -----------------
    • The framework MUST NOT assume any execution environment
    • Tools and connectors are always consumer-provided
    • The returned Agent is fully wired and ready to run
    • No side effects (no global state, no registrations)

    This mirrors mature framework design (e.g., FastAPI, SQLAlchemy),
    where the framework provides structure and guarantees, while the
    application provides domain-specific components.
    """

    @staticmethod
    def create(
        *,
        planner_type: str,
        model: Optional[str],
        connectors: ConnectorManager,
        registry: ToolRegistry,
    ) -> Agent:
        """
        Construct and return a fully initialized TopoMind Agent.

        This method performs **pure assembly only**:
        it does not mutate global state, register tools, or
        create connectors internally.

        Parameters
        ----------
        planner_type : str
            Identifier for the planner backend to use.

            Supported values typically include:
                • "rule"   – deterministic rule-based planner
                • "ollama" – local LLM planner via Ollama
                • "openai" – OpenAI-backed LLM planner

        model : Optional[str]
            Model identifier for LLM-based planners.
            Ignored for non-LLM planners.

            Examples:
                "mistral", "llama3", "gpt-4o-mini"

        connectors : ConnectorManager
            Consumer-provided execution backends.

            Connectors define *how* tools are executed
            (local code, APIs, databases, services, etc.).

        registry : ToolRegistry
            Consumer-provided tool definitions.

            Tools define *what* the agent is allowed to do.
            The framework never invents or assumes tools.

        Returns
        -------
        Agent
            A fully constructed TopoMind Agent instance,
            ready to process user input via `handle_query()`.

        Architectural Notes
        -------------------
        • Planner selection is delegated to `planner.factory`
        • Tool execution is guarded by ToolExecutor firewalls
        • Memory and stability layers are initialized inside Agent
        • No framework defaults leak into consumer infrastructure
        """

        # Planner configuration is framework-owned
        config = AgentConfig(
            planner_type=planner_type,
            model=model,
        )

        # Planner construction (LLM / rule-based / etc.)
        planner = create_planner(config)

        # Execution boundary (tools + connectors)
        executor = ToolExecutor(registry, connectors)

        # Final agent assembly
        return Agent(planner, executor)
class AgentConfig:
    """
    Configuration object for selecting planner backend.
    """

    def __init__(self, planner_type="rule", model=None):
        self.planner_type = planner_type
        self.model = model
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Dict, Any


class ExecutionConnector(ABC):
    """
    Abstract execution backend representing the boundary between
    agent cognition and external deterministic systems.

    A connector is responsible for performing real-world actions such as:
        • Calling APIs
        • Querying databases
        • Running local compute
        • Interacting with enterprise systems

    Architectural Role
    -------------------
    ToolExecutor enforces *policy* (validation, retries, timeouts).
    ExecutionConnector performs the *actual operation*.

    Connectors must:
        • Be deterministic given the same inputs
        • Respect the provided timeout
        • Raise TimeoutError on timeout
        • Raise standard Exceptions for execution failures
        • Never mutate the provided arguments
        • Avoid leaking resources across calls
    """

    @abstractmethod
    def execute(
        self,
        tool_name: str,
        args: Dict[str, Any],
        timeout: int,
    ) -> Any:
        """
        Execute a tool call within the specified timeout.

        Parameters
        ----------
        tool_name : str
            Name of the tool being invoked.

        args : Dict[str, Any]
            Validated input arguments. MUST NOT be mutated.

        timeout : int
            Maximum allowed execution time in seconds.

        Returns
        -------
        Any
            Structured tool output matching the tool's declared output schema.

        Raises
        ------
        TimeoutError
            If execution exceeds the allowed time.

        Exception
            For deterministic execution failures (network errors,
            database failures, etc.).
        """
        raise NotImplementedError

    # ------------------------------------------------------------------
    # Optional Lifecycle Hooks
    # ------------------------------------------------------------------

    def health(self) -> bool:
        """
        Return the connector's health status.

        Used by stability systems to detect degraded or failing backends.
        Default implementation assumes healthy.
        """
        return True

    def shutdown(self) -> None:
        """
        Gracefully release resources (connections, sessions, etc.).

        Called during agent shutdown or connector replacement.
        """
        pass


class FakeConnector(ExecutionConnector):
    """Testing connector that respects tool output schema."""

    def execute(self, tool_name: str, args: Dict[str, Any], timeout: int) -> Any:
        if tool_name == "echo":
            # Must return dict to match output_schema
            return {"text": args.get("text", "")}

        return {"result": f"Tool {tool_name} executed with args {args}"}
from __future__ import annotations

from typing import Dict, Iterable
from threading import RLock

from .base import ExecutionConnector


class ConnectorManager:
    """
    Registry and control layer for execution connectors.

    This component is the final boundary between the agent runtime
    and external systems. It manages connector registration, lookup,
    and lifecycle operations in a thread-safe manner.

    Architectural Role
    -------------------
    ToolExecutor delegates execution to connectors via this manager.
    The manager itself contains no execution logic — it only routes
    requests and manages connector lifecycle.

    Design Properties
    -----------------
    • Thread-safe access using a re-entrant lock  
    • Prevents accidental connector overwrites  
    • Supports bulk registration  
    • Provides observability hooks (health)  
    • Supports graceful shutdown of external resources
    """

    def __init__(self) -> None:
        """Initialize an empty connector registry."""
        self._connectors: Dict[str, ExecutionConnector] = {}
        self._lock = RLock()

    # ------------------------------------------------------------------
    # Registration
    # ------------------------------------------------------------------

    def register(self, name: str, connector: ExecutionConnector) -> None:
        """
        Register a connector under a unique name.

        Parameters
        ----------
        name : str
            Identifier used by tools to reference this connector.

        connector : ExecutionConnector
            Concrete connector implementation.

        Raises
        ------
        ValueError
            If name is invalid or already registered.

        TypeError
            If connector does not implement ExecutionConnector.
        """
        if not name or not isinstance(name, str):
            raise ValueError("Connector must have a valid string name.")

        if not isinstance(connector, ExecutionConnector):
            raise TypeError("Connector must implement ExecutionConnector.")

        with self._lock:
            if name in self._connectors:
                raise ValueError(f"Connector '{name}' already registered.")
            self._connectors[name] = connector

    def register_many(self, connectors: Dict[str, ExecutionConnector]) -> None:
        """
        Atomically register multiple connectors.

        Raises
        ------
        ValueError
            If any connector name already exists.

        TypeError
            If any object does not implement ExecutionConnector.
        """
        with self._lock:
            for name, connector in connectors.items():
                if name in self._connectors:
                    raise ValueError(f"Connector '{name}' already registered.")
                if not isinstance(connector, ExecutionConnector):
                    raise TypeError(f"{name} is not a valid ExecutionConnector.")
            self._connectors.update(connectors)

    # ------------------------------------------------------------------
    # Lookup
    # ------------------------------------------------------------------

    def get(self, name: str) -> ExecutionConnector:
        """
        Retrieve a registered connector.

        Raises
        ------
        KeyError
            If the connector does not exist.
        """
        with self._lock:
            try:
                return self._connectors[name]
            except KeyError:
                raise KeyError(f"Connector '{name}' is not registered.") from None

    def has_connector(self, name: str) -> bool:
        """Return True if a connector with this name exists."""
        with self._lock:
            return name in self._connectors

    def list_connectors(self) -> Iterable[str]:
        """Return the names of all registered connectors."""
        with self._lock:
            return list(self._connectors.keys())

    # ------------------------------------------------------------------
    # Observability & Control
    # ------------------------------------------------------------------

    def health(self) -> Dict[str, bool]:
        """
        Return the health status of all connectors.

        Used by stability monitoring to detect degraded backends.
        """
        with self._lock:
            return {name: conn.health() for name, conn in self._connectors.items()}

    def shutdown_all(self) -> None:
        """
        Gracefully shutdown all connectors.

        Intended for agent shutdown or restart scenarios.
        """
        with self._lock:
            for conn in self._connectors.values():
                try:
                    conn.shutdown()
                except Exception:
                    # Shutdown should not propagate connector errors
                    pass

    def __len__(self) -> int:
        """Return the number of registered connectors."""
        with self._lock:
            return len(self._connectors)
import requests
from typing import Dict, Any
from .base import ExecutionConnector


class OllamaConnector(ExecutionConnector):

    def __init__(self, model: str = "mistral"):
        self.model = model
        self.url = "http://localhost:11434/api/chat"

    def execute(self, tool_name: str, args: Any, **kwargs) -> Dict[str, Any]:
        """
        args may be:
        - {"question": "..."}  (correct)
        - "some text"          (LLM mistake)
        - None
        """

        # Normalize input robustly
        if isinstance(args, dict):
            question = args.get("question")
        elif isinstance(args, str):
            question = args
        else:
            question = None

        # Final fallback
        if not question:
            question = f"Explain: {tool_name}"

        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": question}],
            "stream": False,
        }

        try:
            response = requests.post(
                self.url,
                json=payload,
                timeout=180,
                proxies={"http": None, "https": None},
            )
            response.raise_for_status()

            data = response.json()
            answer = data.get("message", {}).get("content", "").strip()

            return {"answer": answer or "No response generated."}

        except Exception as e:
            return {"answer": f"LLM reasoning failed: {str(e)}"}
class MemoryDecay:
    """
    Computes importance score with reinforcement + age penalty.
    """

    def __init__(self, graph, scorer):
        self._graph = graph
        self._scorer = scorer

    def compute_importance(self, node_id: str) -> float:
        node = self._graph.get_node(node_id)
        age = self._graph.current_turn - node.turn_created
        persistence = self._scorer.score(node_id)

        return (persistence * 2) - (age * 0.1)
from typing import Optional
from .graph import MemoryGraph


class MemoryDeduplicator:
    """
    Prevents redundant nodes from being created.

    NOTE: O(n) scan. Can be replaced with indexing later.
    """

    def __init__(self, graph: MemoryGraph):
        self._graph = graph

    def find_existing(self, type_: str, value) -> Optional[str]:
        for node in self._graph.nodes():
            if node.type == type_ and node.value == value:
                return node.id
        return None
from dataclasses import dataclass


@dataclass(frozen=True)
class Edge:
    """
    Directed relationship between two nodes in the memory graph.
    """

    source: str
    target: str
    relation: str
class MemoryForgetting:
    """
    Removes low-importance episodic nodes while preserving
    structural memory integrity.

    Structural node types (goal, constraint, signal) are never
    pruned automatically because they represent agent state
    and policy context.
    """

    # Node types that should NEVER be auto-forgotten
    PROTECTED_TYPES = {"goal", "constraint", "signal"}

    def __init__(self, graph, decay):
        self._graph = graph
        self._decay = decay

    def prune(self, threshold: float = 0.0) -> None:
        to_remove = []

        for node in list(self._graph.nodes()):

            #  Protect structural memory
            if node.type in self.PROTECTED_TYPES:
                continue

            importance = self._decay.compute_importance(node.id)

            if importance < threshold:
                to_remove.append(node.id)

        if to_remove:
            self._graph.remove_nodes(to_remove)
import uuid
from typing import Dict, List, Iterable, Tuple

from .nodes import Node
from .edges import Edge


class MemoryGraph:
    """
    Semantic knowledge graph for agent memory.

    This class is a *data structure only* — it does not implement
    cognitive policies like decay or forgetting. Those belong to
    MemoryUpdater.

    Nodes are immutable. The graph supports controlled pruning
    and safe state restoration.
    """

    def __init__(self) -> None:
        self._nodes: Dict[str, Node] = {}
        self._edges: List[Edge] = []
        self._turn: int = 0

    # ------------------------------------------------------------------
    # Turn Management
    # ------------------------------------------------------------------

    def new_turn(self) -> None:
        """Advance the logical conversation turn."""
        self._turn += 1

    @property
    def current_turn(self) -> int:
        return self._turn

    # ------------------------------------------------------------------
    # Node Operations
    # ------------------------------------------------------------------

    def add_node(self, type_: str, value) -> str:
        """
        Add a new node to memory.

        Returns
        -------
        str
            ID of the created node.
        """
        node_id = str(uuid.uuid4())
        self._nodes[node_id] = Node(node_id, type_, value, self._turn)
        return node_id

    def get_node(self, node_id: str) -> Node:
        return self._nodes[node_id]

    def get_nodes_by_type(self, type_: str) -> List[Node]:
        return [n for n in self._nodes.values() if n.type == type_]

    def nodes(self) -> Iterable[Node]:
        return self._nodes.values()

    # ------------------------------------------------------------------
    # Edge Operations
    # ------------------------------------------------------------------

    def add_edge(self, source: str, target: str, relation: str) -> None:
        """Add a directed relationship between two nodes."""
        self._edges.append(Edge(source, target, relation))

    def edges(self) -> Iterable[Edge]:
        return self._edges

    # ------------------------------------------------------------------
    # Controlled Pruning (Structural Operation Only)
    # ------------------------------------------------------------------

    def remove_nodes(self, node_ids: List[str]) -> Tuple[List[Node], List[Edge]]:
        """
        Remove nodes and all connected edges safely.

        Returns
        -------
        Tuple[List[Node], List[Edge]]
            The removed nodes and edges (for audit or stability tracking).
        """
        removed_nodes: List[Node] = []
        removed_edges: List[Edge] = []

        # Remove nodes
        for nid in node_ids:
            node = self._nodes.pop(nid, None)
            if node:
                removed_nodes.append(node)

        # Remove edges referencing removed nodes
        remaining_edges = []
        for edge in self._edges:
            if edge.source in node_ids or edge.target in node_ids:
                removed_edges.append(edge)
            else:
                remaining_edges.append(edge)

        self._edges = remaining_edges

        return removed_nodes, removed_edges

    # ------------------------------------------------------------------
    # Persistence Support
    # ------------------------------------------------------------------

    def load_state(
        self,
        turn: int,
        nodes: Dict[str, Node],
        edges: List[Edge],
    ) -> None:
        """
        Replace entire graph state during persistence restore.

        This is the ONLY allowed mutation of internal structures
        from outside normal operations.
        """
        self._turn = turn
        self._nodes = nodes
        self._edges = edges
from dataclasses import dataclass
from typing import Any


@dataclass(frozen=True)
class Node:
    """
    Immutable unit of knowledge stored in the agent memory graph.

    Nodes represent semantic concepts such as:
    entity, goal, constraint, result, assumption, signal.

    Nodes are immutable, but the graph may prune them through
    controlled forgetting policies.
    """

    id: str
    type: str
    value: Any
    turn_created: int
from ..models import Observation
from .semantic_extractor import SemanticExtractor


class ObservationBuilder:

    def __init__(self):
        self.extractor = SemanticExtractor()

    def from_reason_result(self, answer_text: str):
        semantics = self.extractor.extract(answer_text)
        observations = []

        # Concepts
        for c in semantics.get("concepts", []):
            observations.append(
                Observation(
                    source="inference",
                    type="concept",
                    payload=c,
                    metadata={}
                )
            )

        # Facts
        for f in semantics.get("facts", []):
            observations.append(
                Observation(
                    source="inference",
                    type="fact",
                    payload=f,
                    metadata={}
                )
            )

        # Relations
        for r in semantics.get("relations", []):
            observations.append(
                Observation(
                    source="inference",
                    type="relation",
                    payload=(r["source"], r["relation"], r["target"]),
                    metadata={}
                )
            )

        return observations
import json
from .graph import MemoryGraph
from .nodes import Node
from .edges import Edge


class MemoryPersistence:
    """
    Handles serialization and deserialization of memory graph
    and persistence scoring state.
    """

    @staticmethod
    def save(graph: MemoryGraph, scorer, path: str) -> None:
        data = {
            "turn": graph.current_turn,
            "nodes": [node.__dict__ for node in graph.nodes()],
            "edges": [edge.__dict__ for edge in graph.edges()],
            "scores": scorer.export(),
        }

        with open(path, "w") as f:
            json.dump(data, f)

    @staticmethod
    def load(graph: MemoryGraph, scorer, path: str) -> None:
        with open(path) as f:
            data = json.load(f)

        nodes = {n["id"]: Node(**n) for n in data["nodes"]}
        edges = [Edge(**e) for e in data["edges"]]

        graph.load_state(data["turn"], nodes, edges)
        scorer.load(data.get("scores", {}))
from collections import defaultdict
from typing import Dict


class PersistenceScorer:
    """
    Tracks node reinforcement across turns.
    """

    def __init__(self):
        self._scores: Dict[str, int] = defaultdict(int)

    def register_occurrence(self, node_id: str) -> None:
        self._scores[node_id] += 1

    def score(self, node_id: str) -> int:
        return self._scores.get(node_id, 0)

    def export(self):
        return dict(self._scores)

    def load(self, data):
        self._scores.update(data)
import requests
import json


class SemanticExtractor:
    def __init__(self, model="mistral"):
        self.model = model
        self.url = "http://localhost:11434/api/chat"

    def extract(self, text: str):
        prompt = f"""
Extract structured knowledge from the text below.

Return STRICT JSON in this format:

{{
  "concepts": [],
  "facts": [],
  "relations": [
    {{"source": "", "relation": "", "target": ""}}
  ]
}}

Text:
\"\"\"{text}\"\"\"
"""

        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
        }

        try:
            response = requests.post(
                self.url,
                json=payload,
                timeout=120,  # semantic pass can take time
                proxies={"http": None, "https": None},
            )
            response.raise_for_status()

            raw = response.json().get("message", {}).get("content", "").strip()

            # Attempt to parse JSON safely
            try:
                data = json.loads(raw)
            except Exception:
                return {"concepts": [], "facts": [text], "relations": []}

            # Ensure required keys exist
            return {
                "concepts": data.get("concepts", []),
                "facts": data.get("facts", []),
                "relations": data.get("relations", []),
            }

        except Exception:
            # Fallback: store raw answer as a fact
            return {"concepts": [], "facts": [text], "relations": []}
from ..models import Observation
from .graph import MemoryGraph
from .dedup import MemoryDeduplicator
from .persistence_score import PersistenceScorer
from .decay import MemoryDecay
from .forgetting import MemoryForgetting


class MemoryUpdater:
    """
    Central orchestrator for the memory lifecycle.

    This component is the "hippocampus" of the agent — it controls how
    new information is encoded, reinforced, decayed, and forgotten.

    Responsibilities
    ----------------
    • Convert observations into memory nodes
    • Deduplicate knowledge to avoid graph explosion
    • Track reinforcement frequency (persistence scoring)
    • Apply time-based decay to memory strength
    • Periodically prune low-importance memories
    • Serve as the single owner of persistence scoring state

    Architectural Note
    -------------------
    MemoryGraph stores structure.
    MemoryUpdater manages *memory dynamics*.
    """

    def __init__(self, graph: MemoryGraph):
        self._graph = graph

        # Structural layer
        self._dedup = MemoryDeduplicator(graph)

        # Cognitive dynamics
        self._scorer = PersistenceScorer()
        self._decay = MemoryDecay(graph, self._scorer)
        self._forget = MemoryForgetting(graph, self._decay)

    # ------------------------------------------------------------------
    # Persistence Access (Ownership Boundary)
    # ------------------------------------------------------------------

    @property
    def scorer(self) -> PersistenceScorer:
        """
        Expose scorer for persistence loading/saving without
        transferring ownership.
        """
        return self._scorer

    # ------------------------------------------------------------------
    # Memory Ingestion
    # ------------------------------------------------------------------

    def update_from_observation(self, obs: Observation) -> None:
        """
        Integrate a structured observation into memory.

        Flow:
        Observation → Dedup → Node creation/reuse → Reinforcement → Forgetting cycle
        """
        self._graph.new_turn()

        # Deduplicate knowledge
        existing = self._dedup.find_existing(obs.type, obs.payload)
        node_id = existing or self._graph.add_node(obs.type, obs.payload)

        # Reinforce importance
        self._scorer.register_occurrence(node_id)

        # Periodic forgetting cycle (every 5 turns)
        if self._graph.current_turn % 5 == 0:
            self._forget.prune(threshold=-5)
from dataclasses import dataclass, field
from typing import Any, Dict
import time
import uuid


@dataclass(frozen=True)
class Observation:
    """
    A normalized perception unit inside the agent.

    Everything the agent learns (tool outputs, memory reads,
    user input, or system signals) becomes an Observation
    before integration into the memory graph.

    Architectural Role
    ------------------
    Execution → Observation → MemoryUpdater → MemoryGraph

    This abstraction ensures:
    • Consistent memory ingestion
    • Replayability
    • Traceability across cognitive steps
    """

    source: str
    """
    Origin of the observation.

    Examples:
    "user", "tool", "memory", "system"
    """

    type: str
    """
    Semantic category of the observation.

    Examples:
    "entity", "result", "error", "signal"
    """

    payload: Any
    """The actual content observed."""

    metadata: Dict[str, Any] = field(default_factory=dict)
    """Optional structured metadata."""

    # --- System Metadata ---
    timestamp: float = field(default_factory=time.time)
    """Unix timestamp when observation was created."""

    trace_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    """Unique identifier linking this observation to system events."""

    # ------------------------------------------------------------------
    # Debug Representation
    # ------------------------------------------------------------------

    def __repr__(self) -> str:
        return (
            f"Observation(type={self.type}, source={self.source}, "
            f"id={self.trace_id[:8]})"
        )
from dataclasses import dataclass, field
from typing import Dict, Any
import uuid


@dataclass(frozen=True)
class ToolCall:
    """
    Represents a planner-issued instruction to execute a tool.

    This is the *execution intent packet* passed from the planner
    to the ToolExecutor. It contains no execution logic — only
    declarative intent.

    Architectural Role
    ------------------
    Planner → ToolCall → ToolExecutor

    This separation ensures:
    • Deterministic execution boundaries
    • Safe logging and replay
    • Compatibility with persistence and tracing
    """

    tool_name: str
    """Name of the tool to invoke."""

    arguments: Dict[str, Any]
    """Validated input parameters for the tool."""

    # --- System Metadata ---
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    """Unique identifier for tracing this tool call."""

    confidence: float = 1.0
    """
    Planner confidence in this action [0.0–1.0].
    Used for stability analysis and learning signals.
    """

    reason: str = ""
    """Planner explanation for why this tool was selected."""

    # ------------------------------------------------------------------
    # Validation
    # ------------------------------------------------------------------

    def __post_init__(self):
        # Normalize confidence
        object.__setattr__(self, "confidence", max(0.0, min(1.0, float(self.confidence))))

    # ------------------------------------------------------------------
    # Debug Representation
    # ------------------------------------------------------------------

    def __repr__(self) -> str:
        return (
            f"ToolCall(id={self.id[:8]}, tool='{self.tool_name}', "
            f"confidence={self.confidence:.2f})"
        )
from dataclasses import dataclass
from typing import Any, Optional, Literal


@dataclass(frozen=True)
class ToolResult:
    """
    Immutable structured record of a tool execution.

    This object represents a single interaction between the agent
    and an external system via a Tool. It is the canonical execution
    observation passed into memory, stability monitoring, and reasoning.

    A ToolResult is version-aware, ensuring that outputs remain tied
    to the schema contract that produced them. This enables schema
    migration, replay, and long-term system stability.

    Attributes
    ----------
    tool_name : str
        Name of the tool that was executed.

    tool_version : str
        Version of the tool schema used during execution.

    status : {"success", "failure", "blocked"}
        Outcome classification:
            success → tool executed correctly
            failure → tool ran but returned error
            blocked → execution prevented (policy/lookup issue)

    output : Any
        Structured tool output (validated). None if execution failed.

    error : Optional[str]
        Error message when status is failure or blocked.

    latency_ms : int
        Execution time in milliseconds (monotonic).

    stability_signal : float
        Confidence score (0.0–1.0) used by stability system.
        Decreases with retries, anomalies, or degraded performance.
    """

    tool_name: str
    tool_version: str
    status: Literal["success", "failure", "blocked"]
    output: Any
    error: Optional[str]
    latency_ms: int
    stability_signal: float

    # ------------------------------------------------------------------
    # Convenience Properties
    # ------------------------------------------------------------------

    @property
    def is_success(self) -> bool:
        return self.status == "success"

    @property
    def is_failure(self) -> bool:
        return self.status == "failure"

    @property
    def is_blocked(self) -> bool:
        return self.status == "blocked"
"""
Core runtime data models for the TopoMind agent.

These dataclasses define the structured information packets that move
between major system layers (Planner, Executor, Memory, Stability).
"""

from .tool_call import ToolCall
from .tool_result import ToolResult
from .observation import Observation

__all__ = ["ToolCall", "ToolResult", "Observation"]
import json
import uuid
import requests
from typing import List

from ..interface import ReasoningEngine
from ..plan_model import Plan, PlanStep
from ...models.tool_call import ToolCall
from ...tools.schema import Tool


class OllamaPlanner(ReasoningEngine):
    """
    LLM-based planner that decides WHICH TOOL to use.
    It NEVER produces final answers.
    """

    def __init__(self, model: str = "mistral"):
        self.model = model
        self.url = "http://localhost:11434/api/chat"

    def generate_plan(self, user_input: str, signals, tools: List[Tool]) -> Plan:
        tool_desc = "\n".join(
            [f"- {t.name}: {t.description}, inputs={t.input_schema}" for t in tools]
        )

        prompt = f"""
You are the cognitive planning engine of an AI agent.

Your job is to choose the correct tool.

Decision policy:

- Use tool "reason" for:
  * Explanations
  * Knowledge questions
  * Scientific, historical, conceptual topics

- Use other tools ONLY for:
  * Actions
  * Data retrieval
  * Calculations
  * System tasks

- NEVER use "echo" unless user asks to repeat text.

Stable context signals: {signals}

User input:
"{user_input}"

Available tools:
{tool_desc}

Return ONLY JSON:
{{ "tool": "...", "args": {{...}}, "reasoning": "...", "confidence": 0.0-1.0 }}
"""

        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
        }

        response = requests.post(
            self.url,
            json=payload,
            proxies={"http": None, "https": None},
        )

        text = response.json().get("message", {}).get("content", "").strip()

        try:
            result = json.loads(text)

            tool_name = result.get("tool", "echo")

            #  CRITICAL FIX — ensure tool always gets the user question
            args = result.get("args") or {"question": user_input}

            step = PlanStep(
                action=ToolCall(
                    id=str(uuid.uuid4()),
                    tool_name=tool_name,
                    arguments=args,
                ),
                reasoning=result.get("reasoning", "LLM decision"),
                confidence=float(result.get("confidence", 0.7)),
            )

            return Plan(steps=[step], goal="LLM-driven planning")

        except Exception:
            # Fallback if planner output cannot be parsed
            step = PlanStep(
                action=ToolCall(
                    id=str(uuid.uuid4()),
                    tool_name="echo",
                    arguments={"text": "Planner failed"},
                ),
                reasoning="Fallback due to LLM parse failure.",
                confidence=0.2,
            )
            return Plan(steps=[step], goal="Fallback")
import json
from typing import List
from openai import OpenAI

from ..interface import ReasoningEngine
from ..plan_model import Plan, PlanStep
from ...models.tool_call import ToolCall
from ...tools.schema import Tool


class OpenAIPlanner(ReasoningEngine):
    """
    LLM-based planner using OpenAI models.
    """

    def __init__(self, model: str = "gpt-4o-mini"):
        self.client = OpenAI()
        self.model = model

    def generate_plan(self, user_input: str, signals, tools: List[Tool]) -> Plan:
        tool_desc = "\n".join(
            [f"- {t.name}: {t.description}, inputs={t.input_schema}" for t in tools]
        )

        prompt = f"""
You are a planning engine.

User input: "{user_input}"
Stable context: {signals}

Available tools:
{tool_desc}

Return ONLY JSON:
{{ "tool": "...", "args": {{...}}, "reasoning": "...", "confidence": 0.0-1.0 }}
"""

        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
        )

        text = response.choices[0].message.content.strip()

        try:
            result = json.loads(text)

            step = PlanStep(
                action=ToolCall(
                    tool_name=result["tool"],
                    arguments=result["args"],
                ),
                reasoning=result.get("reasoning", "LLM decision"),
                confidence=float(result.get("confidence", 0.7)),
            )

            return Plan(steps=[step], goal="LLM-driven planning")

        except Exception:
            # Safe fallback step
            step = PlanStep(
                action=ToolCall(
                    tool_name="echo",
                    arguments={"text": "Planner failed"},
                ),
                reasoning="Fallback due to LLM parse failure.",
                confidence=0.2,
            )

            return Plan(steps=[step], goal="Fallback")
from .rule_planner import RuleBasedPlanner
from .interface import ReasoningEngine


def create_planner(config) -> ReasoningEngine:
    """
    Factory for constructing the system reasoning engine.

    Planner selection is driven by configuration. Supported types:

    - "rule"    → deterministic rule-based planner
    - "ollama"  → LLM planner using local Ollama models
    - "openai"  → LLM planner using OpenAI API

    Raises
    ------
    ValueError
        If planner type is unknown.
    """

    planner_type = getattr(config, "planner_type", "rule")

    if planner_type == "rule":
        return RuleBasedPlanner()

    if planner_type == "ollama":
        from .adapters.ollama import OllamaPlanner
        return OllamaPlanner(model=config.model)

    if planner_type == "openai":
        from .adapters.openai import OpenAIPlanner
        return OpenAIPlanner(model=config.model)

    raise ValueError(f"Unsupported planner type: {planner_type}")
from abc import ABC, abstractmethod
from typing import Dict, Any, List

from ..tools.schema import Tool
from .plan_model import Plan


class ReasoningEngine(ABC):
    """
    Abstract reasoning interface.

    A ReasoningEngine converts user input + system signals +
    available tool capabilities into a structured execution plan.

    Implementations may be:
    - Rule-based (deterministic)
    - LLM-based (Ollama/OpenAI/etc.)
    - Hybrid planners
    """

    @abstractmethod
    def generate_plan(
        self,
        user_input: str,
        signals: Dict[str, Any],
        tools: List[Tool],
    ) -> Plan:
        """
        Generate a structured plan.

        Parameters
        ----------
        user_input : str
            Raw user request.

        signals : Dict[str, Any]
            System-level signals and memory-derived context.
            Example: stable_entities, constraints, system health.

        tools : List[Tool]
            Tools available for planning. Defines the action space.

        Returns
        -------
        Plan
            Structured reasoning output describing which tool(s)
            to call and why.
        """
        pass
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any, Iterator

from ..models.tool_call import ToolCall


@dataclass
class PlanStep:
    """
    A single reasoning step produced by the planner.

    A PlanStep couples a ToolCall with reasoning metadata that
    explains *why* the planner selected this action.

    This enables:
    • Traceable reasoning
    • Stability monitoring
    • Memory explanation storage
    """

    action: ToolCall
    """Tool action to execute."""

    reasoning: str
    """Planner explanation for the decision."""

    confidence: float = 1.0
    """
    Planner confidence score in range [0.0, 1.0].

    Values outside the range are automatically clamped.
    """

    def __post_init__(self):
        # Safety normalization
        self.confidence = max(0.0, min(1.0, float(self.confidence)))


@dataclass
class Plan:
    """
    Structured output of a ReasoningEngine.

    A Plan represents the planner’s intention over one or more steps.

    Current system executes only the first step, but the structure
    supports future multi-step and branching plans.

    Architectural role:
        Planner output → Agent execution → Memory trace
    """

    steps: List[PlanStep] = field(default_factory=list)
    """Ordered sequence of reasoning steps."""

    goal: Optional[str] = None
    """High-level objective guiding this plan."""

    meta: Dict[str, Any] = field(default_factory=dict)
    """
    Optional planner metadata.

    Examples:
    • model name
    • token usage
    • planner type
    """

    # ------------------------------------------------------------------
    # Convenience Accessors
    # ------------------------------------------------------------------

    @property
    def first_step(self) -> PlanStep:
        """
        Return the first reasoning step.

        Raises
        ------
        ValueError
            If plan contains no steps.
        """
        if not self.steps:
            raise ValueError("Cannot access first_step of empty plan.")
        return self.steps[0]

    def is_empty(self) -> bool:
        """Return True if planner produced no actions."""
        return len(self.steps) == 0

    def size(self) -> int:
        """Return number of steps in the plan."""
        return len(self.steps)

    # ------------------------------------------------------------------
    # Iteration Support (future multi-step execution)
    # ------------------------------------------------------------------

    def __iter__(self) -> Iterator[PlanStep]:
        """Allow iteration over plan steps."""
        return iter(self.steps)
from typing import List

from .interface import ReasoningEngine
from .plan_model import Plan, PlanStep
from ..models.tool_call import ToolCall
from ..tools.schema import Tool


class RuleBasedPlanner(ReasoningEngine):
    """
    Deterministic baseline planner.

    Used when no LLM planner is configured. Provides predictable,
    testable behavior and serves as a safe fallback.
    """

    def generate_plan(
        self,
        user_input: str,
        signals,
        tools: List[Tool],
    ) -> Plan:
        available_tools = {t.name for t in tools}

        # Fallback if echo tool not available
        if "echo" not in available_tools:
            return Plan(
                steps=[],
                goal="No valid tool available",
                meta={"reason": "echo tool missing"},
            )

        stable = signals.get("stable_entities", [])

        # Case 1: Reference persistent memory
        if stable:
            step = PlanStep(
                action=ToolCall(
                    tool_name="echo",
                    args={"text": f"Still talking about: {stable[0]}"},
                ),
                reasoning="Referenced stable entity from memory signals.",
                confidence=0.9,
            )
            return Plan(steps=[step], goal="Continue topic")

        # Case 2: Greeting
        if "hello" in user_input.lower():
            step = PlanStep(
                action=ToolCall(
                    tool_name="echo",
                    args={"text": "Hello from TopoMind Planner!"},
                ),
                reasoning="Greeting intent detected.",
                confidence=1.0,
            )
            return Plan(steps=[step], goal="Respond to greeting")

        # Default behavior
        step = PlanStep(
            action=ToolCall(
                tool_name="echo",
                args={"text": f"You said: {user_input}"},
            ),
            reasoning="Fallback echo behavior.",
            confidence=0.7,
        )

        return Plan(steps=[step], goal="Echo user input")
from __future__ import annotations

from collections import Counter
from typing import Iterable, List, Any, Protocol, runtime_checkable


@runtime_checkable
class SupportsEntityNode(Protocol):
    """Protocol for memory nodes that may represent entities."""
    value: Any


@runtime_checkable
class SupportsMemoryGraph(Protocol):
    """Protocol describing the required memory graph interface."""
    def get_nodes_by_type(self, type_name: str) -> Iterable[SupportsEntityNode]: ...


class PersistenceAnalyzer:
    """
    Detects entities that persist across memory observations.

    Purpose
    -------
    This component helps the agent distinguish:
    - transient mentions (short-term)
    - stable concepts (long-term relevance)

    Persistence is currently frequency-based but can evolve to
    recency- or weight-based scoring without changing this API.
    """

    ENTITY_TYPE = "entity"

    def __init__(self, memory_graph: SupportsMemoryGraph) -> None:
        self._memory = memory_graph

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def persistent_entities(self, threshold: int = 2) -> List[Any]:
        """
        Return entity values appearing at least `threshold` times.

        Parameters
        ----------
        threshold : int
            Minimum occurrences required to consider an entity stable.

        Returns
        -------
        List[Any]
            Stable entity values safe for planner biasing.
        """
        if threshold < 1:
            raise ValueError("threshold must be >= 1")

        nodes = self._safe_entity_nodes()
        values = self._extract_hashable_values(nodes)

        if not values:
            return []

        counts = Counter(values)
        return [value for value, count in counts.items() if count >= threshold]

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------

    def _safe_entity_nodes(self) -> Iterable[SupportsEntityNode]:
        """Safely retrieve entity nodes without propagating memory faults."""
        try:
            nodes = self._memory.get_nodes_by_type(self.ENTITY_TYPE)
            return nodes if nodes is not None else []
        except Exception:
            return []

    def _extract_hashable_values(
        self, nodes: Iterable[SupportsEntityNode]
    ) -> List[Any]:
        """Extract valid, hashable entity values."""
        values: List[Any] = []

        for node in nodes:
            value = getattr(node, "value", None)
            if value is None:
                continue

            try:
                hash(value)
            except TypeError:
                continue

            values.append(value)

        return values
from typing import Dict, Any
from .persistence import PersistenceAnalyzer


class StabilitySignals:
    """
    Aggregates system-level cognitive stability signals.

    These signals are provided to the planner to bias decisions
    toward consistent, long-term context.
    """

    def __init__(self, memory_graph) -> None:
        self._analyzer = PersistenceAnalyzer(memory_graph)

    def extract(self) -> Dict[str, Any]:
        """
        Produce structured stability signals.

        Returns
        -------
        Dict[str, Any]
            Signals safe for planner consumption.
        """
        return {
            "stable_entities": self._analyzer.persistent_entities(),
        }
# topomind/tools/builtin/reason_tool.py

from topomind.tools.schema import Tool

ReasonTool = Tool(
    name="reason",
    description="Use LLM to answer conceptual or knowledge questions",
    input_schema={"question": "string"},
    output_schema={"answer": "string"},
    connector_name="llm",   # connector that talks to Ollama
)
from __future__ import annotations

import time
from typing import Dict, Any

from .registry import ToolRegistry
from ..connectors.manager import ConnectorManager
from ..models import ToolResult
from .validator import ArgumentValidator, ArgumentValidationError
from .output_validator import OutputValidator, OutputValidationError


class ToolExecutor:
    """
    Core execution kernel responsible for running agent tools safely and deterministically.

    This component forms the controlled boundary between agent cognition
    (planning and reasoning) and real-world execution via connectors.

    Execution Flow
    --------------
    Planner → ToolCall → ArgumentValidator → Connector → OutputValidator → ToolResult

    Responsibilities
    ----------------
    • Resolve tool and connector from registries  
    • Enforce argument schema validation (input firewall)  
    • Enforce output schema validation (output firewall)  
    • Apply execution policies (timeout, retries)  
    • Isolate failures from propagating into agent logic  
    • Measure execution latency using a monotonic clock  
    • Produce structured, version-aware ToolResult objects  

    Safety Guarantees
    -----------------
    • Hallucinated parameters are rejected before execution  
    • Malformed tool outputs are blocked from entering memory  
    • Connector failures do not crash the agent loop  
    • Tool contract version is preserved for schema evolution  

    This class contains no business logic and no external system
    interactions beyond delegating to connectors.
    """

    def __init__(self, registry: ToolRegistry, connectors: ConnectorManager) -> None:
        """
        Parameters
        ----------
        registry : ToolRegistry
            Source of tool contracts and schemas.

        connectors : ConnectorManager
            Execution backend routing layer.
        """
        self._registry = registry
        self._connectors = connectors
        self._arg_validator = ArgumentValidator(registry)
        self._out_validator = OutputValidator(registry)

    def execute(self, tool_name: str, args: Dict[str, Any]) -> ToolResult:
        """
        Execute a tool call under controlled runtime policies.

        Parameters
        ----------
        tool_name : str
            Name of the tool to execute.

        args : Dict[str, Any]
            Proposed input arguments (validated before execution).

        Returns
        -------
        ToolResult
            Immutable execution result with version, latency,
            status classification, and stability signal.
        """
        start = time.monotonic()

        try:
            tool = self._registry.get(tool_name)
            connector = self._connectors.get(tool.connector_name)
        except KeyError as e:
            return self._blocked_result(tool_name, str(e))
        except Exception as e:
            return self._blocked_result(tool_name, f"Registry failure: {e}")

        # Argument Validation
        try:
            args = self._arg_validator.validate(tool_name, args)
        except ArgumentValidationError as e:
            return self._failure_result(tool_name, tool.version, f"Invalid arguments: {e}", start)

        max_attempts = tool.max_retries + 1 if tool.retryable else 1
        timeout = tool.timeout_seconds

        for attempt in range(max_attempts):
            try:
                raw_output = self._execute_with_timeout(connector, tool_name, args, timeout)

                # Output Validation
                output = self._out_validator.validate(tool_name, raw_output)

                stability = 1.0 - (attempt * 0.1)
                return self._success_result(tool_name, tool.version, output, start, stability)

            except OutputValidationError as e:
                return self._failure_result(tool_name, tool.version, f"Invalid output: {e}", start)

            except TimeoutError:
                error = f"Execution timed out after {timeout}s"

            except Exception as e:
                error = str(e)

            if attempt >= max_attempts - 1:
                return self._failure_result(tool_name, tool.version, error, start)

        return self._failure_result(tool_name, tool.version, "Unknown execution state", start)

    def _execute_with_timeout(self, connector, tool_name: str, args: Dict[str, Any], timeout: int) -> Any:
        """
        Delegate execution to connector while enforcing timeout policy.
        """
        return connector.execute(tool_name, args, timeout=timeout)

    # ------------------------------------------------------------------
    # Result Builders
    # ------------------------------------------------------------------

    def _success_result(self, tool_name: str, tool_version: str, output: Any, start_time: float, stability: float) -> ToolResult:
        """Build a successful execution result."""
        return ToolResult(
            tool_name=tool_name,
            tool_version=tool_version,
            status="success",
            output=output,
            error=None,
            latency_ms=self._latency_ms(start_time),
            stability_signal=max(0.0, min(1.0, stability)),
        )

    def _failure_result(self, tool_name: str, tool_version: str, error: str, start_time: float) -> ToolResult:
        """Build a failure result caused during execution."""
        return ToolResult(
            tool_name=tool_name,
            tool_version=tool_version,
            status="failure",
            output=None,
            error=error,
            latency_ms=self._latency_ms(start_time),
            stability_signal=0.0,
        )

    def _blocked_result(self, tool_name: str, error: str) -> ToolResult:
        """Build a blocked result (policy or resolution failure)."""
        return ToolResult(
            tool_name=tool_name,
            tool_version="unknown",
            status="blocked",
            output=None,
            error=error,
            latency_ms=0,
            stability_signal=0.0,
        )
    # ------------------------------------------------------------------
    # Public Accessors
    # ------------------------------------------------------------------

    @property
    def registry(self) -> ToolRegistry:
        """
        Read-only access to tool registry.

        The Agent uses this to provide available tools to the planner.
        Execution authority remains inside ToolExecutor.
        """
        return self._registry
    @staticmethod
    def _latency_ms(start_time: float) -> int:
        """Return elapsed time in milliseconds using a monotonic clock."""
        return int((time.monotonic() - start_time) * 1000)
from __future__ import annotations

from typing import Dict, Any

from .registry import ToolRegistry


class OutputValidationError(Exception):
    """Raised when tool output violates declared schema."""
    pass


class OutputValidator:
    """
    Validates tool outputs against declared output schemas.

    This protects memory, planner context, and reasoning layers
    from malformed or unexpected tool responses.
    """

    def __init__(self, registry: ToolRegistry) -> None:
        self._registry = registry

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def validate(self, tool_name: str, output: Any) -> Any:
        """
        Validate tool output. Returns output if valid.

        Raises OutputValidationError if schema is violated.
        """
        schema = self._registry.get_output_schema(tool_name)

        if not isinstance(output, dict):
            raise OutputValidationError("Tool output must be a dictionary.")

        self._check_required(schema, output)
        self._check_unknown(schema, output)
        self._check_types(schema, output)

        return output

    # ------------------------------------------------------------------
    # Validation Steps
    # ------------------------------------------------------------------

    def _check_required(self, schema: Dict[str, Any], output: Dict[str, Any]) -> None:
        missing = [k for k in schema if k not in output]
        if missing:
            raise OutputValidationError(f"Missing output fields: {missing}")

    def _check_unknown(self, schema: Dict[str, Any], output: Dict[str, Any]) -> None:
        extra = [k for k in output if k not in schema]
        if extra:
            raise OutputValidationError(f"Unexpected output fields: {extra}")

    def _check_types(self, schema: Dict[str, Any], output: Dict[str, Any]) -> None:
        for key, expected_type in schema.items():
            value = output[key]

            if not self._matches_type(expected_type, value):
                raise OutputValidationError(
                    f"Output field '{key}' expected type {expected_type}, got {type(value).__name__}"
                )

    # ------------------------------------------------------------------
    # Type Matching
    # ------------------------------------------------------------------

    def _matches_type(self, expected: Any, value: Any) -> bool:
        if isinstance(expected, str):
            return self._string_type_match(expected, value)

        if isinstance(expected, type):
            return isinstance(value, expected)

        return True

    def _string_type_match(self, expected: str, value: Any) -> bool:
        mapping = {
            "string": str,
            "int": int,
            "float": float,
            "bool": bool,
            "dict": dict,
            "list": list,
        }

        expected_type = mapping.get(expected.lower())
        if expected_type is None:
            return True

        return isinstance(value, expected_type)
from __future__ import annotations

from typing import Dict, List, Any, Iterable
from threading import RLock

from .schema import Tool


class ToolRegistry:
    """
    Authoritative registry of all tools available to the agent.

    This forms the capability boundary: if a tool is not registered here,
    it is not executable by the agent.
    """

    def __init__(self) -> None:
        self._tools: Dict[str, Tool] = {}
        self._lock = RLock()  # Future-proof for concurrent planners/executors

    # ------------------------------------------------------------------
    # Registration
    # ------------------------------------------------------------------

    def register(self, tool: Tool) -> None:
        """
        Register a tool.

        Raises
        ------
        ValueError
            If tool name already exists or is invalid.
        """
        if not tool.name or not isinstance(tool.name, str):
            raise ValueError("Tool must have a valid string name.")

        with self._lock:
            if tool.name in self._tools:
                raise ValueError(f"Tool '{tool.name}' is already registered.")
            self._tools[tool.name] = tool

    def register_many(self, tools: Iterable[Tool]) -> None:
        """Register multiple tools atomically."""
        with self._lock:
            for tool in tools:
                if tool.name in self._tools:
                    raise ValueError(f"Tool '{tool.name}' is already registered.")
            for tool in tools:
                self._tools[tool.name] = tool

    # ------------------------------------------------------------------
    # Lookup
    # ------------------------------------------------------------------

    def get(self, tool_name: str) -> Tool:
        """Retrieve a tool by name."""
        with self._lock:
            try:
                return self._tools[tool_name]
            except KeyError:
                raise KeyError(f"Tool '{tool_name}' is not registered.") from None

    def has_tool(self, tool_name: str) -> bool:
        """Check whether a tool exists."""
        with self._lock:
            return tool_name in self._tools

    def list_tools(self) -> List[Tool]:
        """
        Return a copy of registered tools to prevent external mutation.
        """
        with self._lock:
            return list(self._tools.values())

    def list_tool_names(self) -> List[str]:
        """Return all registered tool names."""
        with self._lock:
            return list(self._tools.keys())

    def __len__(self) -> int:
        with self._lock:
            return len(self._tools)

    # ------------------------------------------------------------------
    # Schema Access (Contract Authority)
    # ------------------------------------------------------------------

    def get_input_schema(self, tool_name: str) -> Dict[str, Any]:
        """Return declared input schema."""
        return self.get(tool_name).input_schema

    def get_output_schema(self, tool_name: str) -> Dict[str, Any]:
        """Return declared output schema."""
        return self.get(tool_name).output_schema
from dataclasses import dataclass, field
from typing import Dict, Any, List


@dataclass(frozen=True)
class Tool:
    """
    Versioned declarative contract describing an agent capability.

    A Tool defines WHAT action can be performed, while execution details
    are delegated to a Connector. This object serves as the canonical
    contract between all runtime layers:

        Planner → ArgumentValidator → ToolExecutor → Connector → Memory

    A Tool is immutable and versioned. Any change to its input or output
    schema constitutes a contract change and must increment `version`.
    Historical versions are preserved for schema migration and replay.

    Design Principles
    -----------------
    - Declarative: contains no execution logic
    - Deterministic: immutable once created
    - Versioned: enables schema evolution without breaking memory
    - Policy-aware: carries runtime constraints (timeouts, retries)
    - Observable: tagged for monitoring and reliability analysis

    Attributes
    ----------
    name : str
        Unique identifier of the tool. Used for planning and dispatch.

    description : str
        Human- and model-readable explanation of tool purpose.

    connector_name : str
        Name of the Connector responsible for execution.

    input_schema : Dict[str, Any]
        Structured definition of expected arguments.

    output_schema : Dict[str, Any]
        Structured definition of returned data.

    version : str
        Semantic version of the tool contract.

    timeout_seconds : int
        Maximum allowed execution time.

    retryable : bool
        Indicates whether transient failures may be retried.

    max_retries : int
        Maximum retry attempts if retryable.

    side_effect : bool
        Whether tool mutates external state.

    tags : List[str]
        Classification and observability tags.
    """

    # ------------------------------------------------------------------
    # Core Identity
    # ------------------------------------------------------------------

    name: str
    description: str
    connector_name: str

    # ------------------------------------------------------------------
    # Schemas (Contract Layer)
    # ------------------------------------------------------------------

    input_schema: Dict[str, Any]
    output_schema: Dict[str, Any]

    # ------------------------------------------------------------------
    # Schema Evolution
    # ------------------------------------------------------------------

    version: str = "1.0.0"

    # ------------------------------------------------------------------
    # Runtime Policy Metadata
    # ------------------------------------------------------------------

    timeout_seconds: int = 10
    retryable: bool = True
    max_retries: int = 2
    side_effect: bool = False

    # ------------------------------------------------------------------
    # Observability
    # ------------------------------------------------------------------

    tags: List[str] = field(default_factory=list)

    # ------------------------------------------------------------------
    # Derived Properties
    # ------------------------------------------------------------------

    @property
    def key(self) -> str:
        """
        Unique identifier combining tool name and version.

        Used by SchemaRegistry to track historical contract versions.
        """
        return f"{self.name}:{self.version}"
from typing import Any, Dict


class SchemaMigrator:
    """
    Migrates tool outputs between schema versions.
    """

    def __init__(self):
        self._migrations = {}

    def register_migration(self, tool_name: str, from_v: str, to_v: str, func):
        self._migrations[(tool_name, from_v, to_v)] = func

    def migrate(self, tool_name: str, from_v: str, to_v: str, data: Dict[str, Any]):
        if from_v == to_v:
            return data

        key = (tool_name, from_v, to_v)
        if key not in self._migrations:
            raise ValueError(f"No migration path {key}")

        return self._migrations[key](data)
from typing import Dict, Tuple
from .schema import Tool


class SchemaRegistry:
    """
    Stores historical versions of tool schemas.
    """

    def __init__(self):
        self._schemas: Dict[Tuple[str, str], Tool] = {}

    def register(self, tool: Tool):
        key = (tool.name, tool.version)
        self._schemas[key] = tool

    def get(self, tool_name: str, version: str) -> Tool:
        return self._schemas[(tool_name, version)]
from __future__ import annotations

from typing import Dict, Any, Tuple

from .registry import ToolRegistry


class ArgumentValidationError(Exception):
    """Raised when tool arguments violate schema."""
    pass


class ArgumentValidator:
    """
    Validates tool arguments against tool input schemas.

    This prevents hallucinated parameters and enforces
    deterministic contracts before execution.
    """

    def __init__(self, registry: ToolRegistry) -> None:
        self._registry = registry

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def validate(self, tool_name: str, args: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate arguments for a tool call.

        Returns sanitized args or raises ArgumentValidationError.
        """
        schema = self._registry.get_input_schema(tool_name)

        if not isinstance(args, dict):
            raise ArgumentValidationError("Arguments must be a dictionary.")

        self._check_required(schema, args)
        self._check_unknown(schema, args)
        self._check_types(schema, args)

        return args  # sanitized (future: coercion here)

    # ------------------------------------------------------------------
    # Validation Steps
    # ------------------------------------------------------------------

    def _check_required(self, schema: Dict[str, Any], args: Dict[str, Any]) -> None:
        missing = [k for k in schema if k not in args]
        if missing:
            raise ArgumentValidationError(f"Missing required arguments: {missing}")

    def _check_unknown(self, schema: Dict[str, Any], args: Dict[str, Any]) -> None:
        extra = [k for k in args if k not in schema]
        if extra:
            raise ArgumentValidationError(f"Unknown arguments: {extra}")

    def _check_types(self, schema: Dict[str, Any], args: Dict[str, Any]) -> None:
        for key, expected_type in schema.items():
            value = args[key]

            if not self._matches_type(expected_type, value):
                raise ArgumentValidationError(
                    f"Argument '{key}' expected type {expected_type}, got {type(value).__name__}"
                )

    # ------------------------------------------------------------------
    # Type Matching
    # ------------------------------------------------------------------

    def _matches_type(self, expected: Any, value: Any) -> bool:
        """
        Supports simple string type hints or Python types.
        """
        if isinstance(expected, str):
            return self._string_type_match(expected, value)

        if isinstance(expected, type):
            return isinstance(value, expected)

        return True  # Unknown schema type → don't block

    def _string_type_match(self, expected: str, value: Any) -> bool:
        mapping = {
            "string": str,
            "int": int,
            "float": float,
            "bool": bool,
            "dict": dict,
            "list": list,
        }

        expected_type = mapping.get(expected.lower())
        if expected_type is None:
            return True  # unknown schema spec → allow

        return isinstance(value, expected_type)
"""
TopoMind Framework
==================

Public API surface for the TopoMind agent framework.
"""

from .app import TopoMindApp

__all__ = [
    "TopoMindApp",
]
